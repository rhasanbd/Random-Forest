{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest Feature Selecion\n",
    "\n",
    "In this notebook, we perform feature selecion using the Random Forest model.\n",
    "\n",
    "**Feature Selection** is a very useful Machine Learning technique that we often use. The Random Forest model is very handy to get a quick understanding of what features actually matter. This helps to perform feature selection.\n",
    "\n",
    "The Random Forest model enables us to perform feature selection by finding the **relative importance of each feature**. \n",
    "\n",
    "A Random Forest measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). \n",
    "\n",
    "More precisely, it is a **weighted average**, where each node’s weight is equal to the number of training samples that are associated with it.\n",
    "\n",
    "Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. \n",
    "\n",
    "We can access the result using the $feature\\_importances\\_$ attribute of the model.\n",
    "\n",
    "In this notebook, we train an optimal Random Forest model on the Iris dataset. Then, use the model to determine feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "\n",
    "We use the Iris dataset, which is a multivariate data set. \n",
    "\n",
    "This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica\n",
    "\n",
    "There are 4 features: \n",
    "- sepal length (cm)\n",
    "- sepal width (cm)\n",
    "- petal length (cm)\n",
    "- petal width (cm)\n",
    "\n",
    "Total number of samples: 150\n",
    "\n",
    "The dataset is also known as Fisher's Iris data set as it was introduced by the British statistician and biologist Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\".\n",
    "\n",
    "\n",
    "<img src=\"https://cse.unl.edu/~hasan/IrisFlowers.png\" width=800 height=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key Values: \n",
      " ['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']\n",
      "\n",
      "Feature Names: \n",
      " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "Target Names: \n",
      " ['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "# See the key values\n",
    "print(\"\\nKey Values: \\n\", list(iris.keys()))\n",
    "\n",
    "# The feature names\n",
    "print(\"\\nFeature Names: \\n\", list(iris.feature_names))\n",
    "\n",
    "# The target names\n",
    "print(\"\\nTarget Names: \\n\", list(iris.target_names))\n",
    "\n",
    "# The target values (codes)\n",
    "#print(\"\\nTarget Values: \\n\", list(iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Matrix (X) and the 1D Target Array (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "\n",
      "X data type:  float64\n",
      "y data type:  int64\n"
     ]
    }
   ],
   "source": [
    "# Feature matrix\n",
    "X = iris[\"data\"]\n",
    "\n",
    "# Target Array\n",
    "y = iris[\"target\"]\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"\\nX data type: \", X.dtype)\n",
    "print(\"y data type: \", y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We use some of the hyperparameters of the RandomForestClassifier class for model selection. \n",
    "\n",
    "For a full list of the hyperparameters visit: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "\n",
    "- n_estimators : The number of trees in the forest. Default=10\n",
    "\n",
    "\n",
    "- criterion : The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific. Default=”gini”\n",
    "\n",
    "\n",
    "- max_depth : The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Default=None\n",
    "\n",
    "\n",
    "- min_samples_split : The minimum number of samples required to split an internal node: Default=2\n",
    "\n",
    "        -- If int, then consider min_samples_split as the minimum number.\n",
    "        \n",
    "        -- If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "        \n",
    "        \n",
    "        \n",
    "- min_samples_leaf : The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. Default=1\n",
    "\n",
    "\n",
    "         -- If int, then consider min_samples_leaf as the minimum number.\n",
    "         \n",
    "         -- If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "\n",
    "\n",
    "- max_features : The number of features to consider when looking for the best split. Default=\"auto\".\n",
    "\n",
    "        -- If int, then consider max_features features at each split.\n",
    "        \n",
    "        -- If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "        \n",
    "        -- If “auto”, then max_features=sqrt(n_features).\n",
    "        \n",
    "        -- If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "        \n",
    "        -- If “log2”, then max_features=log2(n_features).\n",
    "        \n",
    "        -- If None, then max_features=n_features.\n",
    "\n",
    "\n",
    "- max_leaf_nodes : Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. Default=None\n",
    "\n",
    "\n",
    "- class_weight : Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Default=None\n",
    "\n",
    "         -- The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "\n",
    "\n",
    "- oob_score : Whether to use out-of-bag samples to estimate the generalization accuracy. Default=False\n",
    "\n",
    "        -- When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting oob_score=True. Then, we can get the score of the training dataset obtained using an out-of-bag estimate (use the $oob\\_score\\_$ attribute)\n",
    "        \n",
    "        \n",
    "- bootstrap : Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. Default=True\n",
    " \n",
    "\n",
    "- verbose : Controls the verbosity when fitting and predicting. Default=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1152 candidates, totalling 3456 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 560 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2960 tasks      | elapsed:   34.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (F1 score): 0.966667\n",
      "Optimal Hyperparameter Values:  {'max_depth': 2, 'max_leaf_nodes': 10, 'min_samples_leaf': 7, 'n_estimators': 100}\n",
      "\n",
      "\n",
      "CPU times: user 5.83 s, sys: 268 ms, total: 6.1 s\n",
      "Wall time: 40.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 3456 out of 3456 | elapsed:   40.6s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {'n_estimators': [10, 20, 50, 100],\n",
    "              'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "              'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "              'max_leaf_nodes': [2, 5, 10, 15]}\n",
    "\n",
    "dt_clf = RandomForestClassifier(criterion=\"gini\", max_features=\"auto\", class_weight=\"balanced\", \n",
    "                                oob_score=True)\n",
    "\n",
    "dt_clf_cv = GridSearchCV(dt_clf, param_grid, scoring='f1_micro', cv=3, verbose=1, n_jobs=-1)\n",
    "dt_clf_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = dt_clf_cv.best_params_\n",
    "\n",
    "print(\"Best Score (F1 score): %f\" % dt_clf_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9416666666666667\n",
      "Test Accuracy:  1.0\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_clf = RandomForestClassifier(criterion=\"gini\", max_features=\"auto\", class_weight=\"balanced\", \n",
    "                                oob_score=True, **params_optimal)\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_predicted = forest_clf.predict(X_train)\n",
    "\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, y_train_predicted))\n",
    "\n",
    "y_test_predicted = forest_clf.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Below we determine the importance of the Iris features using the Random Forest model's $feature\\_importances\\_$ property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) : 0.12\n",
      "sepal width (cm) : 0.01\n",
      "petal length (cm) : 0.37\n",
      "petal width (cm) : 0.50\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(iris.feature_names)):\n",
    "    print(\"%10s : %.2f\" % (iris.feature_names[i], forest_clf.feature_importances_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "We observe the two most important features are: \n",
    "- Petal length\n",
    "- Petal width"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
