{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest - Study of Randomness and Extracted Features\n",
    "\n",
    "\n",
    "In this notebook, we study two bagging-based techniques of increasing randomness in the Random Forst model. Also, we investigate the performance of the Random Forest model trained on extracted features. We perform three tasks on a large and complex classification dataset.\n",
    "\n",
    "- Task 1: Train a Random Forest model by using the bagging method on raw features.\n",
    "- Task 2: Train a Random Forest model by using the Extra-Trees method on raw features.\n",
    "- Task 3: Train a Random Forest model by using the bagging method on **extracted features**.\n",
    "\n",
    "\n",
    "## Bagging & Extra-Trees Method\n",
    "\n",
    "For task 1 and 2, we use raw features to train the Random Forest model using the bagging method and its augmented version.\n",
    "\n",
    "Bagging stands for bootstrap aggregation. Using bagging, we train many decision trees on different random subsets of the training set. The sampling is performed with replacement. The bagging method induces additional randomness in the trees by learning from a random subset of features.\n",
    "\n",
    "The extremely randomized trees or **Extra-Trees** method is created by augmenting the bagging method. The Extra-Trees method increases randomness by learning from random thresholds of each feature.\n",
    "\n",
    "The Extra-Trees based model is much faster to train than the vanilla bagging method based Random Forests. This is due to the fact that finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree.\n",
    "\n",
    "\n",
    "## Random Forest Trained with Extracted Features\n",
    "\n",
    "For task 3, we investigate whether the Random Forest model can be trained effectively using extracted features. We use a dimensionality reduction technique, i.e., PCA, for feature extraction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: MNIST\n",
    "\n",
    "We use the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.\n",
    "\n",
    "There are 70,000 images. Each image is 28x28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black).\n",
    "\n",
    "Thus, each image has 784 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Create Data Matrix (X) and the Label Vector (y)\n",
    "\n",
    "We load the data from the API. However, note that we don't scale the data. The Random Forest model doesn't require standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "# Load data using Scikit-Learn\n",
    "mnist = fetch_openml('mnist_784', cache=False)\n",
    "\n",
    "X = mnist[\"data\"].astype('float64')\n",
    "y = mnist[\"target\"].astype('int64')\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Train a Random Forest by using the Bagging Method on Raw Features\n",
    "\n",
    "We did not do hyperparameter tuning to select the optimal Random Forest model.\n",
    "\n",
    "Separately we experimented with several hyperparameter values and found the near-optimal model, which is trained below. Ideally we should have performed a grid search for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training took 136.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.9692142857142857\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1370    0    1    0    2    3    6    0    5    0]\n",
      " [   0 1563    5    4    3    0    1    1    2    1]\n",
      " [   3    2 1406    6    3    1    5    8    8    1]\n",
      " [   2    1   22 1360    0   16    0   11   19    4]\n",
      " [   1    0    1    0 1314    0    5    4    3   22]\n",
      " [   1    2    3   12    2 1186   12    0    8    5]\n",
      " [   6    2    1    0    3    8 1363    0    4    0]\n",
      " [   3    5   22    1    7    0    0 1399    3   18]\n",
      " [   0    9    3    5    5    5    3    0 1318   20]\n",
      " [   4    0    5   19   14    5    2   14    8 1290]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1387\n",
      "           1       0.99      0.99      0.99      1580\n",
      "           2       0.96      0.97      0.97      1443\n",
      "           3       0.97      0.95      0.96      1435\n",
      "           4       0.97      0.97      0.97      1350\n",
      "           5       0.97      0.96      0.97      1231\n",
      "           6       0.98      0.98      0.98      1387\n",
      "           7       0.97      0.96      0.97      1458\n",
      "           8       0.96      0.96      0.96      1368\n",
      "           9       0.95      0.95      0.95      1361\n",
      "\n",
      "    accuracy                           0.97     14000\n",
      "   macro avg       0.97      0.97      0.97     14000\n",
      "weighted avg       0.97      0.97      0.97     14000\n",
      "\n",
      "\n",
      "Score of the training dataset obtained using an out-of-bag estimate:  0.9708571428571429\n",
      "\n",
      "\n",
      "CPU times: user 12min 53s, sys: 14.1 s, total: 13min 7s\n",
      "Wall time: 2min 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "forest_clf = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", max_features=\"auto\", \n",
    "                                    max_depth=32, class_weight=\"balanced\", oob_score=True, \n",
    "                                    verbose=1, n_jobs=-1)\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "training_forest_clf = t1 - t0\n",
    "\n",
    "print(\"Random Forest Training took {:.2f}s\".format(training_forest_clf))\n",
    "\n",
    "y_test_predicted = forest_clf.predict(X_test)\n",
    "accuracy_forest_clf = accuracy_score(y_test, y_test_predicted)\n",
    "print(\"\\nTest Accuracy: \", accuracy_forest_clf)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nScore of the training dataset obtained using an out-of-bag estimate: \", forest_clf.oob_score_)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Train a Random Forest by using the Extra-Trees Method on Raw Features\n",
    "\n",
    "We use Scikit-Learn’s ExtraTreesClassifier class for creating an Extra-Trees model. It is identical to the RandomForestClassifier class. Similarly, the Extra TreesRegressor class has the same API as the RandomForestRegressor class.\n",
    "\n",
    "### Note:\n",
    "By deault the ExtraTreesClassifier \"bootstrap\" hyperparameter is set to False. As a consequence, the whole dataset is used to build each tree. To use the bagging method, we need to explicity set it to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   52.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tree Training took 104.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    1.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy:  0.9675714285714285\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[1369    0    1    0    2    2    6    0    7    0]\n",
      " [   0 1562    6    3    3    1    1    1    2    1]\n",
      " [   3    3 1401    7    3    1    5    8   11    1]\n",
      " [   2    1   22 1358    0   17    2   10   18    5]\n",
      " [   2    0    1    0 1314    0    4    4    2   23]\n",
      " [   1    1    3   10    2 1188   12    1    8    5]\n",
      " [   8    2    1    0    3    7 1364    0    2    0]\n",
      " [   3    7   22    0    7    0    0 1389    6   24]\n",
      " [   0   10    4    9    6    6    3    0 1310   20]\n",
      " [   5    2    4   19   10    6    2   14    8 1291]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1387\n",
      "           1       0.98      0.99      0.99      1580\n",
      "           2       0.96      0.97      0.96      1443\n",
      "           3       0.97      0.95      0.96      1435\n",
      "           4       0.97      0.97      0.97      1350\n",
      "           5       0.97      0.97      0.97      1231\n",
      "           6       0.97      0.98      0.98      1387\n",
      "           7       0.97      0.95      0.96      1458\n",
      "           8       0.95      0.96      0.96      1368\n",
      "           9       0.94      0.95      0.95      1361\n",
      "\n",
      "    accuracy                           0.97     14000\n",
      "   macro avg       0.97      0.97      0.97     14000\n",
      "weighted avg       0.97      0.97      0.97     14000\n",
      "\n",
      "\n",
      "Score of the training dataset obtained using an out-of-bag estimate:  0.9700535714285714\n",
      "\n",
      "\n",
      "CPU times: user 8min 36s, sys: 15.5 s, total: 8min 52s\n",
      "Wall time: 1min 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=1000, criterion=\"gini\", max_features=\"auto\", \n",
    "                                       max_depth=32, class_weight=\"balanced\", oob_score=True, \n",
    "                                       bootstrap=True, verbose=1, n_jobs=-1)\n",
    "extra_trees_clf.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "training_extra_tree = t1 - t0\n",
    "\n",
    "print(\"Extra Tree Training took {:.2f}s\".format(training_extra_tree))\n",
    "\n",
    "\n",
    "y_test_predicted = extra_trees_clf.predict(X_test)\n",
    "accuracy_extra_trees = accuracy_score(y_test, y_test_predicted)\n",
    "print(\"\\nTest Accuracy: \", accuracy_extra_trees)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nScore of the training dataset obtained using an out-of-bag estimate: \", extra_trees_clf.oob_score_)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 & 2: Summary of Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Running-Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (1000 trees)</td>\n",
       "      <td>0.969214</td>\n",
       "      <td>136.028079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extra-Trees (1000 trees)</td>\n",
       "      <td>0.967571</td>\n",
       "      <td>104.795368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Classifier  Accuracy  Running-Time\n",
       "0  Random Forest (1000 trees)  0.969214    136.028079\n",
       "1    Extra-Trees (1000 trees)  0.967571    104.795368"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[\"Random Forest (1000 trees)\", accuracy_forest_clf, training_forest_clf], \n",
    "        [\"Extra-Trees (1000 trees)\", accuracy_extra_trees, training_extra_tree]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"Classifier\", \"Accuracy\", \"Running-Time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 & 2: Comparative Understanding\n",
    "\n",
    "From the above results, we see that the performance of Extra-Trees and Random Forest model are comparable. \n",
    "\n",
    "- Extra-Trees model is **faster at the cost of slightly higher bias** (i.e., slightly smaller test accuracy). The speed in training comes from using random threshold then searching for an optimal threshold.\n",
    "- The performance difference is insignificant.\n",
    "\n",
    "\n",
    "### RandomForestClassifier vs. ExtraTreesClassifier\n",
    "\n",
    "It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an ExtraTreesClassifier. \n",
    "\n",
    "Generally, the only way to know is to try both and compare them using cross-validation (and tuning the hyperparameters using grid search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Train a Random Forest model by Using the Bagging Method on Extracted Features\n",
    "\n",
    "The goal of this task is to determine whether a Random Forest model trained on extracted features exhibit better performance. In other words, we want to see whether the Random Forest model learns effectively on extracted features.\n",
    " \n",
    "We use the **Principle Component Analysis (PCA)** dimensionality reduction technique to extract a low-dimensional features. Then, train the Random Forest model using the extracted features.\n",
    "\n",
    "We apply the PCA to project the MNIST dataset (784 features) to a lower dimensional space by retaining maximum variance (95%).\n",
    "\n",
    "The PCA **extracts 154 features**, which we use to train a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Principle Components (Extracted Features):  154\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(\"Number of Principle Components (Extracted Features): \", pca.n_components_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   39.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (PCA) Training took 223.10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy (PCA):  0.9492857142857143\n",
      "\n",
      "Test Confusion Matrix (PCA):\n",
      "[[1360    0    4    1    2    5   10    1    4    0]\n",
      " [   0 1551   10    6    2    0    4    2    5    0]\n",
      " [   8    4 1361   14   10    2    4    7   30    3]\n",
      " [   2    0   22 1323    1   23    7   14   33   10]\n",
      " [   1    1    6    1 1299    0    8    1    6   27]\n",
      " [   5    2    6   26    9 1152   13    3    4   11]\n",
      " [   9    1    5    1    6   11 1351    0    3    0]\n",
      " [   2    6   20    2   14    1    0 1387    8   18]\n",
      " [   2   11   11   31   10   21    5    2 1259   16]\n",
      " [   8    1    4   25   34    8    2   25    7 1247]]\n",
      "\n",
      "Classification Report (PCA):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      1387\n",
      "           1       0.98      0.98      0.98      1580\n",
      "           2       0.94      0.94      0.94      1443\n",
      "           3       0.93      0.92      0.92      1435\n",
      "           4       0.94      0.96      0.95      1350\n",
      "           5       0.94      0.94      0.94      1231\n",
      "           6       0.96      0.97      0.97      1387\n",
      "           7       0.96      0.95      0.96      1458\n",
      "           8       0.93      0.92      0.92      1368\n",
      "           9       0.94      0.92      0.93      1361\n",
      "\n",
      "    accuracy                           0.95     14000\n",
      "   macro avg       0.95      0.95      0.95     14000\n",
      "weighted avg       0.95      0.95      0.95     14000\n",
      "\n",
      "\n",
      "Score of the training (PCA) dataset obtained using an out-of-bag estimate:  0.9521785714285714\n",
      "\n",
      "\n",
      "CPU times: user 26min 24s, sys: 9.18 s, total: 26min 33s\n",
      "Wall time: 3min 44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "forest_clf_pca = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", max_features=\"auto\", \n",
    "                                    max_depth=32, class_weight=\"balanced\", oob_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "forest_clf_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "training_forest_clf_pca = t1 - t0\n",
    "\n",
    "print(\"Random Forest (PCA) Training took {:.2f}s\".format(training_forest_clf_pca))\n",
    "\n",
    "y_test_predicted_pca = forest_clf_pca.predict(X_test_pca)\n",
    "accuracy_forest_clf_pca = accuracy_score(y_test, y_test_predicted_pca)\n",
    "print(\"\\nTest Accuracy (PCA): \", accuracy_forest_clf_pca)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix (PCA):\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_pca))\n",
    "\n",
    "print(\"\\nClassification Report (PCA):\")\n",
    "print(classification_report(y_test, y_test_predicted_pca))\n",
    "\n",
    "print(\"\\nScore of the training (PCA) dataset obtained using an out-of-bag estimate: \", forest_clf_pca.oob_score_)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 & 3: Summary of Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Running-Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (784 raw features)</td>\n",
       "      <td>0.969214</td>\n",
       "      <td>136.028079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest (154 extracted features)</td>\n",
       "      <td>0.949286</td>\n",
       "      <td>223.095504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Classifier  Accuracy  Running-Time\n",
       "0        Random Forest (784 raw features)  0.969214    136.028079\n",
       "1  Random Forest (154 extracted features)  0.949286    223.095504"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[\"Random Forest (784 raw features)\", accuracy_forest_clf, training_forest_clf], \n",
    "        [\"Random Forest (154 extracted features)\", accuracy_forest_clf_pca, training_forest_clf_pca]]\n",
    "\n",
    "pd.DataFrame(data, columns=[\"Classifier\", \"Accuracy\", \"Running-Time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 & 3 Observation: Random Forest with Extracted Features\n",
    "\n",
    "We observe that extracted features obtained with PCA **lowers the performance** of a Random Forest classifier. This is due to the fact that extracted features don't meaningfully combine to produce effective desion rules.\n",
    "\n",
    "Machine Learning algorithms that create decision rules by composing knowledge (e.g., decision tree uses a combination of the features to contruct a decision rule) don't benefit from dimensionality reduction. \n",
    "\n",
    "Thus, **we shouldn't use PCA with Decision Tree or Random Forest**."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
