{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forest - Training and Feature Selection\n",
    "\n",
    "In this notebook, we perform two tasks.\n",
    "- Task 1: Train a Random Forest model optimally by using the **bagging** method.\n",
    "- Task 2: Perform feature selection usin the Random Forest model.\n",
    "\n",
    "We motivate the bagging method by emphasizing the better generalizability of the Random Forest model.\n",
    "\n",
    "\n",
    "## How Does the Random Forest Model Improve Generalizability?\n",
    "\n",
    "The Random Forest achieves better generalizability by reducing the variance of individual Decision Trees.\n",
    "\n",
    "To reduce variance, the component trees are all designed to be **randomly different from one another**. This leads to **de-correlation between the individual tree predictions** and, in turn, to improved generalization.\n",
    "\n",
    "Forest randomness also helps achieve high robustness with respect to **noisy data**.\n",
    "\n",
    "\n",
    "## Randomization Techniques for Training Random Forests\n",
    "\n",
    "Randomness is injected into the trees during the training phase. Two of the most popular randomness injection techniques are:\n",
    "     - Random training dataset sampling   \n",
    "     - Randomized node optimization (RNO)\n",
    "\n",
    "Unlike bagging, RNO uses use all available training data and controls the randomness by varying the number of features for training.\n",
    "\n",
    "\n",
    "## Randomization Technique: Random Training Dataset Sampling \n",
    "\n",
    "\n",
    "For random sampling from the training dataset, the bootstrapping technique is used. In bootstrapping, we use the same training algorithm for every predictor, but to **train them on different random subsets** of the training set. \n",
    "- When sampling is performed with replacement, this method is called bagging. Bagging stands for **bootstrap aggregation**.\n",
    "- When sampling is performed without replacement, it is called pasting.\n",
    "\n",
    "\n",
    "<img src=\"http://engineering.unl.edu/images/uploads/Bagging.png\" width=700 height=400>\n",
    "\n",
    "\n",
    "## Bagging vs Pasting\n",
    "\n",
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on. Thus, bagging ends up with a slightly higher bias than pasting. The extra diversity also means that the predictors end up being less correlated, so the ensemble’s variance is reduced. Overall, bagging often results in better models, thus it is generally preferred. \n",
    "\n",
    "\n",
    "## Training Random Forest using the Bagging Method\n",
    "\n",
    "We use Scikit-Learn's RandomForestClassifier to implement the bagging method for training the Random Forest model. We induce randomness in following two ways:\n",
    "\n",
    "- Sampling from dataset: learn from random subsets of the data\n",
    "- Sampling from features: use a random subset of features to consider when looking for the best split\n",
    "\n",
    "## Augmented Bagging\n",
    "\n",
    "To induce additional randomness, the bagging method can be augmented as follows.\n",
    "\t- Learn from random thresholds of each feature (using extremely randomized trees or **Extra-Trees**).\n",
    "\n",
    "We implenment the Extra-Trees method in the next notebook.\n",
    "\n",
    "\n",
    "\n",
    "# Feature Selecion\n",
    "\n",
    "The Random Forest model can be used to perform feature selecion. It enables us to detrmine the **relative importance of each feature**. \n",
    "\n",
    "A Random Forest measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest). \n",
    "\n",
    "More precisely, it is a **weighted average**, where each node’s weight is equal to the number of training samples that are associated with it.\n",
    "\n",
    "Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. \n",
    "\n",
    "We can access the result using the $feature\\_importances\\_$ attribute of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "\n",
    "We use the Iris dataset, which is a multivariate data set. \n",
    "\n",
    "This is a famous dataset that contains the sepal and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica\n",
    "\n",
    "There are 4 features: \n",
    "- sepal length (cm)\n",
    "- sepal width (cm)\n",
    "- petal length (cm)\n",
    "- petal width (cm)\n",
    "\n",
    "Total number of samples: 150\n",
    "\n",
    "The dataset is also known as Fisher's Iris data set as it was introduced by the British statistician and biologist Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\".\n",
    "\n",
    "\n",
    "<img src=\"http://engineering.unl.edu/images/uploads/IrisFlowers.png\" width=800 height=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key Values: \n",
      " ['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']\n",
      "\n",
      "Feature Names: \n",
      " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "Target Names: \n",
      " ['setosa', 'versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "# See the key values\n",
    "print(\"\\nKey Values: \\n\", list(iris.keys()))\n",
    "\n",
    "# The feature names\n",
    "print(\"\\nFeature Names: \\n\", list(iris.feature_names))\n",
    "\n",
    "# The target names\n",
    "print(\"\\nTarget Names: \\n\", list(iris.target_names))\n",
    "\n",
    "# The target values (codes)\n",
    "#print(\"\\nTarget Values: \\n\", list(iris.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Matrix (X) and the 1D Target Array (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "\n",
      "X data type:  float64\n",
      "y data type:  int64\n"
     ]
    }
   ],
   "source": [
    "# Feature matrix\n",
    "X = iris[\"data\"]\n",
    "\n",
    "# Target Array\n",
    "y = iris[\"target\"]\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"\\nX data type: \", X.dtype)\n",
    "print(\"y data type: \", y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Train a Random Forest Classifier Model Optimally Using the Bagging Method \n",
    "\n",
    "A Random Forest is an **ensemble of Decision Trees**. It is generally trained via the bagging method (or sometimes pasting). \n",
    "\n",
    "The bagging method induces randomess (to implement tree diversity) by training the same Decision Tree on different random samples from the data.\n",
    "\n",
    "In addition to the sample-based randomness, a Random Forest model induces feature-based randomness.\n",
    "\n",
    "Instead of searching for the very best feature when splitting a node, it searches for the best feature among a **random subset of features**. This results in a greater tree diversity, which trades a higher bias for a lower variance, generally yielding an overall better model. \n",
    "\n",
    "\n",
    "### Scikit-Learn for Using the Bagging Method to Train a Random Forest Model\n",
    "\n",
    "There are two ways to use the bagging method to train a Random Forest classifier.\n",
    "\n",
    "- Using the BaggingClassifier class create an objet and pass it a decision tree created from the DecisionTreeClassifier class.\n",
    "\n",
    "- Create a Random Forest object using the **RandomForestClassifier class**.\n",
    "\n",
    "The second approach is more convenient and optimized for Decision Trees. Similarly, there is a RandomForestRegressor class for regression tasks. \n",
    "\n",
    "\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "We use some of the hyperparameters of the RandomForestClassifier class for model selection. \n",
    "\n",
    "For a full list of the hyperparameters visit: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "\n",
    "- n_estimators : The number of trees in the forest. Default=10\n",
    "\n",
    "\n",
    "- criterion : The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific. Default=”gini”\n",
    "\n",
    "\n",
    "- max_depth : The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Default=None\n",
    "\n",
    "\n",
    "- min_samples_split : The minimum number of samples required to split an internal node: Default=2\n",
    "\n",
    "        -- If int, then consider min_samples_split as the minimum number.\n",
    "        \n",
    "        -- If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "        \n",
    "        \n",
    "        \n",
    "- min_samples_leaf : The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. Default=1\n",
    "\n",
    "\n",
    "         -- If int, then consider min_samples_leaf as the minimum number.\n",
    "         \n",
    "         -- If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "\n",
    "\n",
    "- max_features : The number of features to consider when looking for the best split. Default=\"auto\".\n",
    "\n",
    "        -- If int, then consider max_features features at each split.\n",
    "        \n",
    "        -- If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "        \n",
    "        -- If “auto”, then max_features=sqrt(n_features).\n",
    "        \n",
    "        -- If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "        \n",
    "        -- If “log2”, then max_features=log2(n_features).\n",
    "        \n",
    "        -- If None, then max_features=n_features.\n",
    "\n",
    "\n",
    "- max_leaf_nodes : Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. Default=None\n",
    "\n",
    "\n",
    "- class_weight : Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Default=None\n",
    "\n",
    "         -- The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "\n",
    "\n",
    "- oob_score : Whether to use out-of-bag samples to estimate the generalization accuracy. Default=False\n",
    "\n",
    "        -- When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting oob_score=True. Then, we can get the score of the training dataset obtained using an out-of-bag estimate (use the $oob\\_score\\_$ attribute)\n",
    "        \n",
    "        \n",
    "- bootstrap : Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. Default=True\n",
    " \n",
    "\n",
    "- verbose : Controls the verbosity when fitting and predicting. Default=0\n",
    "\n",
    "\n",
    "\n",
    "### Out-of-Bag (oob) Evaluation\n",
    "\n",
    "When using the bagging method, some instances may be sampled several times for any given model, while **others may not be sampled at all**. In general, only about 63% of the training instances are sampled on average for each model. The remaining 37% of the training instances are not sampled. These are called the out-of-bag (oob) instances. \n",
    "\n",
    "The bagging method-based Random Forest can be evaluated using oob instances. We don't need a separate validation set. The oob evaluation can be used as an estimation for the test accuracy of the model.\n",
    "\n",
    "- For implementing the oob evaluation, we need to set oob_score=True.\n",
    "\n",
    "\n",
    "### Note:\n",
    "We use default setting for the following **two key hyperparameters**.\n",
    "\n",
    "- Sampling from data: bootstrap=True (it ensures that the bagging method is used)\n",
    "\n",
    "- Sampling from features: max_features=\"auto\" (selects a subset of the features, i.e., sqrt(n_features) features to consider when looking for the best split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2304 candidates, totalling 6912 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 782 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-1)]: Done 2282 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 4382 tasks      | elapsed:   53.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (F1 score): 0.966667\n",
      "Optimal Hyperparameter Values:  {'criterion': 'entropy', 'max_depth': 3, 'max_leaf_nodes': 15, 'min_samples_leaf': 3, 'n_estimators': 100}\n",
      "\n",
      "\n",
      "CPU times: user 10.4 s, sys: 369 ms, total: 10.8 s\n",
      "Wall time: 1min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 6912 out of 6912 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {'n_estimators': [10, 20, 50, 100],\n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "              'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "              'max_leaf_nodes': [2, 5, 10, 15]}\n",
    "\n",
    "\n",
    "'''\n",
    "Create a Random Forest classifier:\n",
    "- Uses out-of-bag samples to estimate the generalization accuracy\n",
    "- Uses the values of y to automatically adjust weights inversely proportionalto class frequencies \n",
    "   in the input data \n",
    "'''\n",
    "dt_clf = RandomForestClassifier(class_weight=\"balanced\", oob_score=True)\n",
    "\n",
    "dt_clf_cv = GridSearchCV(dt_clf, param_grid, scoring='f1_micro', cv=3, verbose=1, n_jobs=-1)\n",
    "dt_clf_cv.fit(X_train, y_train)\n",
    "\n",
    "params_optimal = dt_clf_cv.best_params_\n",
    "\n",
    "print(\"Best Score (F1 score): %f\" % dt_clf_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9583333333333334\n",
      "Out of Bag (oob) Score:  0.9416666666666667\n",
      "\n",
      "Test Accuracy:  1.0\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_clf = RandomForestClassifier(class_weight=\"balanced\", oob_score=True, **params_optimal)\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_predicted = forest_clf.predict(X_train)\n",
    "\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, y_train_predicted))\n",
    "\n",
    "print(\"Out of Bag (oob) Score: \", forest_clf.oob_score_)\n",
    "\n",
    "y_test_predicted = forest_clf.predict(X_test)\n",
    "\n",
    "print(\"\\nTest Accuracy: \", accuracy_score(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Feature Selection\n",
    "\n",
    "Below we determine the importance of the Iris features using the trained Random Forest model's $feature\\_importances\\_$ property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) : 0.11\n",
      "sepal width (cm) : 0.01\n",
      "petal length (cm) : 0.39\n",
      "petal width (cm) : 0.48\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(iris.feature_names)):\n",
    "    print(\"%10s : %.2f\" % (iris.feature_names[i], forest_clf.feature_importances_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "We observe the two most important features are: \n",
    "- Petal length\n",
    "- Petal width"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
